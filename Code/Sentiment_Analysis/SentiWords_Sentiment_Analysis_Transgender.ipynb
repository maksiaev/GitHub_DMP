{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8af1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alexander Maksiaev\n",
    "# Purpose: Sentiment analysis of textbooks using Sentiwords\n",
    "\n",
    "\n",
    "\n",
    "# SentiWords 1.1 Dictionary \n",
    "# Dictionary Source:\n",
    "# Gatti, Lorenzo, Marco Guerini, and Marco Turchi. \n",
    "# \"SentiWords: Deriving a high precision and high coverage lexicon for sentiment analysis.\" \n",
    "# IEEE Transactions on Affective Computing 7.4 (2016): 409-421."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507f9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv \n",
    "import docx\n",
    "from english_words import get_english_words_set\n",
    "import enchant\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "\n",
    "web2lowerset = get_english_words_set(['web2'], lower=True)\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "textbook_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Textbook_Dump_Transgender\"\n",
    "\n",
    "textbooks = os.listdir(textbook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01600558",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(textbook_dir)\n",
    "\n",
    "# Function to get full text\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return fullText\n",
    "\n",
    "# Dictionary filled with text for all the books, minus the titles\n",
    "title_text = {}\n",
    "for book in textbooks:\n",
    "    total_text = getText(book)\n",
    "    text_without_title = total_text[4:]\n",
    "    for piece in text_without_title:\n",
    "        if piece == '':\n",
    "            text_without_title.remove(piece) # Does not get rid of all whitespace, but ah well.\n",
    "#     text_without_title.remove('')\n",
    "    title_text[book] = text_without_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03bfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82391430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from each book\n",
    "\n",
    "stopwords_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\GitHub_DMP\\Stop_Words\" \n",
    "\n",
    "os.chdir(stopwords_dir)\n",
    "\n",
    "f = open(\"stop_words_english_modified.txt\", \"r\", encoding=\"utf-8\")\n",
    "stopwords = []\n",
    "for text in f:\n",
    "    text = text.replace('\\n', '')\n",
    "    stopwords.append(text)\n",
    "\n",
    "\n",
    "punc = '''!()[]{};:'-\"\\,<>./?@#$%^&*_~''' # Must include \"-\" in words... or not?\n",
    "\n",
    "# Function to clean up text and remove stopwords\n",
    "def clean(book):\n",
    "    text_list = title_text[book]\n",
    "    new_text_list = []\n",
    "    \n",
    "    # Clean up text\n",
    "    for text in text_list:\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = text.split(' ')\n",
    "        new_text_list.append(text)\n",
    "\n",
    "\n",
    "    newer_text_list = []\n",
    "    \n",
    "    # Remove punctuation\n",
    "    for sentence in new_text_list:\n",
    "        for word in sentence:\n",
    "            for char in punc:\n",
    "                if char in word:\n",
    "                    word = word.replace(char, '')\n",
    "            newer_text_list.append(word)\n",
    "                \n",
    "    # Remove stop words\n",
    "    newest_text_list = []\n",
    "    for words in newer_text_list:\n",
    "        if words not in stopwords:\n",
    "            newest_text_list.append(words)\n",
    "\n",
    "    # Remove blanks\n",
    "    for w in newest_text_list:\n",
    "        if len(w) == 0:\n",
    "            newest_text_list.remove(w)\n",
    "            \n",
    "    return newest_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06de2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all the books with their clean, stopword-less counterparts\n",
    "\n",
    "clean_texts = {}\n",
    "for book in title_text:\n",
    "    newest_text_list = clean(book)\n",
    "    clean_texts[book] = newest_text_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614a35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef4a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_to_sentiwordnet = {\n",
    "    \"NN\": \"n\",\n",
    "    \"VB\": \"v\",\n",
    "    \"JJ\": \"a\",\n",
    "    \"RB\": \"r\",\n",
    "}\n",
    "\n",
    "def get_sentiment(article):\n",
    "\n",
    "    word_sum = len(article)\n",
    "    \n",
    "    tagged_sentence_words = nltk.pos_tag(article)\n",
    "\n",
    "    # Already filtered out stopwords, so no need to do it again\n",
    "    \n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    subj_scores = []\n",
    "\n",
    "    for word, pos in tagged_sentence_words:\n",
    "        \n",
    "        swn_pos = nltk_to_sentiwordnet.get(pos[:2], None)\n",
    "        \n",
    "        synsets = list(swn.senti_synsets(word.lower(), pos=swn_pos))\n",
    "\n",
    "        \n",
    "#         print(word)\n",
    "        \n",
    "#         if word == \"autism\" or word == \"autistic\" or word == \"asperger's\":\n",
    "#             word_sum -= 1\n",
    "#             swn_pos = None\n",
    "#             synsets = 0\n",
    "    \n",
    "        if swn_pos == None:\n",
    "            continue\n",
    "    \n",
    "            \n",
    "        if len(synsets) == 0:\n",
    "            continue\n",
    "    \n",
    "        #print(\"{}:\".format(word))\n",
    "        for synset in synsets[:1]:\n",
    "            pos_scores.append(synset.pos_score())\n",
    "            neg_scores.append(synset.neg_score())\n",
    "            subj_scores.append(1 - synset.obj_score())\n",
    "            \n",
    "    # We only care about positive and negative scores\n",
    "    positives = sum(pos_scores)\n",
    "    negatives = sum(neg_scores)\n",
    "            \n",
    "        \n",
    "    # Divide by length of text\n",
    "    balanced_sentiment = (positives - negatives) \n",
    "    \n",
    "    # Scale sentiment to the +4 -4 dichotomy \n",
    "    scaled_sentiment = balanced_sentiment * 4\n",
    "    \n",
    "    final_sentiment = scaled_sentiment / word_sum\n",
    "        \n",
    "#     pos_score = np.average(pos_scores, weights=subj_scores)\n",
    "#     neg_score = np.average(neg_scores, weights=subj_scores)\n",
    "#     neut_score = np.mean(subj_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930ca122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sentiment(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])\n",
    "\n",
    "# Balance out negative and positive scores (positive, negative, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f7b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f347e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abn_Barlow_04_Transgender_v2.docx', 'Abn_Barlow_05_Transgender_v2.docx', 'Abn_Barlow_06_Transgender_v2.docx', 'Abn_Barlow_07_Transgender_v2.docx', 'Abn_Barlow_08_Transgender.docx', 'Abn_Brown_05_Transgender.docx', 'Abn_Comer_05_Transgender_v2.docx', 'Abn_Comer_06_Transgender.docx', 'Abn_Comer_07_Transgender.docx', 'Abn_Comer_08_Transgender_v2.docx', 'Abn_Comer_09_Transgender.docx', 'Abn_Hooley_13_Transgender.docx', 'Abn_Hooley_14_Transgender_v2.docx', 'Abn_Hooley_15_Transgender_v2.docx', 'Abn_Hooley_16_Transgender_v2.docx', 'Abn_Hooley_17_Transgender.docx', 'Abn_Kearney_06_Transgender.docx', 'Abn_Mash_02_Transgender.docx', 'Abn_Mash_05_Transgender.docx', 'Abn_Mash_06_Transgender.docx', 'Abn_Nevid_08_Transgender_v2.docx', 'Abn_Nevid_09_Transgender.docx', 'Abn_Nevid_10_Transgender.docx', 'Abn_Nolen-Hoeksema_03_Transgender.docx', 'Abn_Nolen-Hoeksema_04_Transgender.docx', 'Abn_Nolen-Hoeksema_05_Transgender.docx', 'Abn_Nolen-Hoeksema_06_Transgender.docx', 'Abn_Nolen-Hoeksema_07_Transgender.docx', 'Abn_Sue_07_Transgender_v2.docx', 'Abn_Sue_08_Transgender_v2.docx', 'Abn_Sue_09_Transgender_v2.docx', 'Abn_Sue_10_Transgender_v2.docx', 'Abn_Sue_11_Transgender.docx', 'Abn_Whitbourne_04_Transgender.docx', 'Abn_Whitbourne_05_Transgender_v2.docx', 'Abn_Whitbourne_06_Transgender.docx', 'Abn_Whitbourne_07_Transgender_v2.docx', 'Abn_Whitbourne_08_Transgender.docx', 'Devo_Berger_05_Transgender.docx', 'Devo_Berger_06_Transgender.docx', 'Devo_Berger_07_Transgender.docx', 'Devo_Berger_08_Transgender.docx', 'Devo_Berger_09_Transgender.docx', 'Devo_Berger_10_Transgender.docx', 'Devo_Berk_08_Transgender.docx', 'Devo_Bornstein_02_Transgender.docx', 'Devo_Bornstein_04_Transgender_v2.docx', 'Devo_Bornstein_05_Transgender_v2.docx', 'Devo_Bornstein_07_Transgender.docx', 'Devo_Feldman_03_Transgender.docx', 'Devo_Feldman_04_Transgender_v2.docx', 'Devo_Feldman_05_Transgender_v2.docx', 'Devo_Feldman_06_Transgender_v2.docx', 'Devo_Feldman_07_Transgender.docx', 'Devo_Feldman_08_Transgender.docx', 'Devo_Gonzalez-Mena_10_Transgender.docx', 'Devo_Gonzalez-Mena_11_Transgender.docx', 'Devo_Kail_02_Transgender.docx', 'Devo_Kail_05_Transgender.docx', 'Devo_Kail_06_Transgender.docx', 'Devo_Kail_07_Transgender.docx', 'Devo_Miller_06_Transgender.docx', 'Devo_Newman_08_Transgender.docx', 'Devo_Newman_09_Transgender.docx', 'Devo_Newman_10_Transgender_v2.docx', 'Devo_Newman_11_Transgender.docx', 'Devo_Newman_12_Transgender.docx', 'Devo_Newman_13_Transgender.docx', 'Devo_Santrock_11_Transgender.docx', 'Devo_Santrock_12_Transgender.docx', 'Devo_Santrock_13_Transgender.docx', 'Devo_Santrock_14_Transgender.docx', 'Devo_Santrock_15_Transgender_V2.docx', 'Devo_Santrock_16_Transgender.docx', 'Devo_Sigelman_05_Transgender.docx', 'Devo_Sigelman_07_Transgender.docx', 'Devo_Sigelman_08_Transgender.docx', 'Devo_Sigelman_09_Transgender.docx', 'GS_Andersen_07_Transgender_v2.docx', 'GS_Andersen_08_Transgender_v2.docx', 'GS_Andersen_09_Transgender.docx', 'GS_Brannon_05_Transgender_v2.docx', 'GS_Brannon_06_Transgender.docx', 'GS_Brannon_07_Transgender.docx', 'GS_Healey_03_Transgender_v2.docx', 'GS_Healey_04_Transgender.docx', 'GS_Healey_05_Transgender.docx', 'GS_Helgeson_03_Transgender_v2.docx', 'GS_Helgeson_04_Transgender_v2.docx', 'GS_Helgeson_05_Transgender.docx', 'GS_Kimmel_04_Transgender_v2.docx', 'GS_Kimmel_05_Transgender_v2.docx', 'GS_Kimmel_06_Transgender.docx', 'GS_Newman_01_Transgender_v2.docx', 'GS_Newman_02_Transgender_v2.docx', 'GS_Newman_03_Transgender.docx', 'GS_Robinson_02_Transgender.docx', 'GS_Robinson_03_Transgender_v2.docx', 'GS_Robinson_04_Transgender.docx', 'GS_Rothenberg_08_Transgender_v2.docx', 'GS_Rothenberg_09_Transgender_v2.docx', 'GS_Rothenberg_10_Transgender.docx', 'GS_Wood_10_Transgender_v2.docx', 'GS_Wood_11_Transgender_v2.docx', 'GS_Wood_12_Transgender.docx', 'HS_Carroll_03_Transgender.docx', 'HS_Carroll_04_Transgender.docx', 'HS_Carroll_05_Transgender.docx', 'HS_Crooks_11_Transgender.docx', 'HS_Crooks_12_Transgender_v2.docx', 'HS_Crooks_13_Transgender.docx', 'HS_Greenberg_04_Transgender.docx', 'HS_Greenberg_05_Transgender_v2.docx', 'HS_Greenberg_06_Transgender.docx', 'HS_Hock_02_Transgender.docx', 'HS_Hock_03_Transgender.docx', 'HS_Hock_04_Transgender.docx', 'HS_Hyde_11_Transgender_v2.docx', 'HS_Hyde_12_Transgender_v2.docx', 'HS_Hyde_13_Transgender.docx', 'HS_LeVay_01_Transgender.docx', 'HS_LeVay_02_Transgender.docx', 'HS_LeVay_03_Transgender.docx', 'HS_Yarber_07_Transgender.docx', 'HS_Yarber_08_Transgender.docx', 'HS_Yarber_09_Transgender.docx', 'Intro_Bernstein_06_Transgender.docx', 'Intro_Bernstein_07_Transgender_v2.docx', 'Intro_Bernstein_08_Transgender.docx', 'Intro_Bernstein_09_Transgender_v2.docx', 'Intro_Bernstein_10_Transgender.docx', 'Intro_Coon_10_Transgender_v2.docx', 'Intro_Coon_11_Transgender.docx', 'Intro_Coon_12_Transgender_v2.docx', 'Intro_Coon_13_Transgender_v2.docx', 'Intro_Coon_14_Transgender.docx', 'Intro_Griggs_01_Transgender.docx', 'Intro_Griggs_04_Transgender.docx', 'Intro_Griggs_05_Transgender.docx', 'Intro_Kalat_08_Transgender.docx', 'Intro_Kalat_11_Transgender.docx', 'Intro_Morris_07_Transgender_v2.docx', 'Intro_Morris_09_Transgender.docx', 'Intro_Morris_10_Transgender.docx', 'Intro_Morris_11_Transgender.docx', 'Intro_Myers_07_Transgender.docx', 'Intro_Myers_08_Transgender.docx', 'Intro_Myers_09_Transgender.docx', 'Intro_Myers_10_Transgender.docx', 'Intro_Myers_11_Transgender.docx', 'Intro_Nevid_01_Transgender.docx', 'Intro_Nevid_02_Transgender_v2.docx', 'Intro_Nevid_03_Transgender_v2.docx', 'Intro_Nevid_04_Transgender.docx', 'Intro_Nevid_05_Transgender.docx', 'Intro_Rathus_01_Transgender.docx', 'Intro_Rathus_02_Transgender.docx', 'Intro_Rathus_03_Transgender.docx', 'Intro_Rathus_04_Transgender.docx', 'Intro_Rathus_05_Transgender.docx', 'Intro_Wade_08_Transgender_v2.docx', 'Intro_Wade_09_Transgender.docx', 'Intro_Wade_11_Transgender_v2.docx', 'Intro_Wade_12_Transgender.docx', 'Intro_Weiten_07_Transgender.docx', 'Intro_Weiten_08_Transgender_v2.docx', 'Intro_Weiten_09_Transgender_v2.docx', 'Intro_Weiten_10_Transgender.docx', 'Neuro_Bear_03_Transgender.docx', 'Neuro_Bear_04_Transgender.docx', 'Neuro_Breedlove_05_Transgender.docx', 'Neuro_Breedlove_06_Transgender.docx', 'Neuro_Breedlove_07_Transgender.docx', 'Neuro_Breedlove_08_Transgender.docx', 'Neuro_Carlson_09_Transgender.docx', 'Neuro_Carlson_10_Transgender_v2.docx', 'Neuro_Carlson_11_Transgender_v2.docx', 'Neuro_Carlson_12_Transgender.docx', 'Neuro_Garrett_02_Transgender.docx', 'Neuro_Garrett_03_Transgender.docx', 'Neuro_Garrett_04_Transgender_v2.docx', 'Neuro_Garrett_05_Transgender.docx', 'Neuro_Haines_05_Transgender.docx', 'Neuro_Johnson_04_Transgender.docx', 'Neuro_Kalat_12_Transgender.docx', 'Neuro_Kolb_07_Transgender.docx', 'Neuro_Pinel_07_Transgender.docx', 'Neuro_Pinel_08_Transgender.docx', 'Neuro_Pinel_09_Transgender_v2.docx', 'Neuro_Pinel_10_Transgender.docx', 'Neuro_Reisberg_06_Transgender.docx', 'Socl_Aronson_08_Transgender.docx', 'Socl_Aronson_09_Transgender.docx', 'Socl_Baumeister_02_Transgender.docx', 'Socl_Baumeister_04_Transgender.docx', 'Socl_Branscombe_12_Transgender.docx', 'Socl_Branscombe_13_Transgender_v2.docx', 'Socl_Branscombe_14_Transgender.docx', 'Socl_Franzoi_05_Transgender.docx', 'Socl_Franzoi_06_Transgender.docx', 'Socl_Franzoi_07_Transgender.docx', 'Socl_Gilovich_02_Transgender.docx', 'Socl_Gilovich_03_Transgender.docx', 'Socl_Gilovich_04_Transgender.docx', 'Socl_Gruman_02_Transgender.docx', 'Socl_Gruman_03_Transgender.docx', 'Socl_Kassin_09_Transgender.docx', 'Socl_Kassin_10_Transgender.docx', 'Socl_Myers_10_Transgender.docx', 'Socl_Myers_11_Transgender.docx', 'Socl_Myers_12_Transgender.docx', 'Socl_Rogers_02_Transgender.docx', 'Socl_Rogers_03_Transgender_v2.docx', 'Socl_Rogers_04_Transgender.docx', 'Socl_Zastrow_08_Transgender.docx', 'Socl_Zastrow_09_Transgender.docx', 'Socl_Zastrow_10_Transgender.docx', 'Spcl_Friend_02_Transgender.docx', 'Spcl_Friend_03_Transgender.docx', 'Spcl_Friend_04_Transgender.docx', 'Spcl_Friend_05_Transgender.docx', 'Spcl_Gargiulo_06_Transgender.docx', 'Spcl_Hardman_10_Transgender.docx', 'Spcl_Hardman_12_Transgender.docx', 'Spcl_Heward_10_Transgender.docx', 'Spcl_Heward_11_Transgender.docx', 'Spcl_Kuder_05_Transgender.docx', 'Spcl_Lewis_09_Transgender.docx', 'Spcl_Overton_08_Transgender.docx', 'Spcl_Smith_04_Transgender.docx', 'Spcl_Smith_05_Transgender.docx', 'Spcl_Smith_07_Transgender.docx', 'Spcl_Turnbull_04_Transgender.docx', 'Spcl_Turnbull_05_Transgender.docx', 'Spcl_Turnbull_06_Transgender.docx', 'Spcl_Turnbull_07_Transgender.docx', 'Spcl_Turnbull_08_Transgender.docx', 'Spcl_Vaughn_07_Transgender.docx']\n"
     ]
    }
   ],
   "source": [
    "# Now, find sentiment score of all of the books\n",
    "\n",
    "dmp_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\GitHub_DMP\\Results\"\n",
    "\n",
    "os.chdir(dmp_dir)\n",
    "\n",
    "print(textbooks)\n",
    "\n",
    "sentiments = {}\n",
    "for textbook in textbooks:\n",
    "    book = clean_texts[textbook]\n",
    "    sentiment = get_sentiment(book)\n",
    "    sentiments[textbook] = sentiment\n",
    "    \n",
    "all_sentiments = pd.DataFrame(sentiments, index = [0])\n",
    "\n",
    "all_sentiments.to_csv('sentiments_sentiwords_3_15_2024_transgender.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44255b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
