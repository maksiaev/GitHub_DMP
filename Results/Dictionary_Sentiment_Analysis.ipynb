{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a95c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alexander Maksiaev\n",
    "# Purpose: Sentiment analysis of textbooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b97008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv \n",
    "import docx\n",
    "from english_words import get_english_words_set\n",
    "import enchant\n",
    "import inflect\n",
    "\n",
    "web2lowerset = get_english_words_set(['web2'], lower=True)\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "textbook_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Textbook_Dump\"\n",
    "\n",
    "textbooks = os.listdir(textbook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353ef867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Dictionaries\"\n",
    "\n",
    "dictionary_location = os.chdir(dictionary_dir)\n",
    "\n",
    "# Clean up the dictionary\n",
    "\n",
    "# dictionary_clean = []\n",
    "# with open(\"SentiWords_1.1_Modified.txt\", newline = '\\n') as dictionary: \n",
    "#     dictionary_reader = csv.reader(dictionary, delimiter='\\t')\n",
    "#     # Skip heading\n",
    "#     for skip in range(26):\n",
    "#         next(dictionary_reader)\n",
    "#     # Only include words that have scores\n",
    "#     for i in dictionary_reader:\n",
    "#         if i[1] != \"0\": # If the word has a score\n",
    "#             dictionary_clean.append(i)\n",
    "\n",
    "# # More cleanup\n",
    "# for i in dictionary_clean:\n",
    "#     word = i[0]\n",
    "#     if \"_\" in word:\n",
    "#         word = word.replace(\"_\", \" \")\n",
    "#     if \"#\" in word:\n",
    "#         hashtag = word.index(\"#\")\n",
    "#         word = word[:hashtag]\n",
    "#     i[0] = word\n",
    "#     i[1] = float(i[1])\n",
    "    \n",
    "# print(dictionary_clean[0:1000])\n",
    "        \n",
    "dictionary_clean = []\n",
    "with open(\"final_dictionary.txt\", newline = '\\n') as dictionary: \n",
    "    dictionary_reader = csv.reader(dictionary, delimiter='\\t')\n",
    "    # Only include words that have scores\n",
    "    for i in dictionary_reader:\n",
    "        if i[1] != \"0\": # If the word has a score\n",
    "            dictionary_clean.append(i)\n",
    "\n",
    "# More cleanup\n",
    "for i in dictionary_clean:\n",
    "#     word = i[0]\n",
    "#     if \"_\" in word:\n",
    "#         word = word.replace(\"_\", \" \")\n",
    "#     if \"#\" in word:\n",
    "#         hashtag = word.index(\"#\")\n",
    "#         word = word[:hashtag]\n",
    "#     i[0] = word\n",
    "    i[1] = float(i[1])\n",
    "    \n",
    "    \n",
    "# print(dictionary_clean[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f408fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Organize names of textbooks into lists of disciplines\n",
    "# abn = []\n",
    "# devo = []\n",
    "# intro = []\n",
    "# neuro = []\n",
    "# socl = []\n",
    "# spcl = []\n",
    "# for book in textbooks:\n",
    "#     if \"Abn_\" in book:\n",
    "#         abn.append(book)\n",
    "#     elif \"Devo_\" in book:\n",
    "#         devo.append(book)\n",
    "#     elif \"Intro_\" in book:\n",
    "#         intro.append(book)\n",
    "#     elif \"Neuro_\" in book:\n",
    "#         neuro.append(book)\n",
    "#     elif \"Socl_\" in book:\n",
    "#         socl.append(book)\n",
    "#     else:\n",
    "#         spcl.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611b0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(abn)\n",
    "\n",
    "# print(dictionary_clean[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8b6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(textbook_dir)\n",
    "\n",
    "# Function to get full text\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return fullText\n",
    "\n",
    "# Dictionary filled with text for all the books, minus the titles\n",
    "title_text = {}\n",
    "for book in textbooks:\n",
    "    total_text = getText(book)\n",
    "    text_without_title = total_text[4:]\n",
    "    for piece in text_without_title:\n",
    "        if piece == '':\n",
    "            text_without_title.remove(piece) # Does not get rid of all whitespace, but ah well.\n",
    "#     text_without_title.remove('')\n",
    "    title_text[book] = text_without_title\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617b841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(title_text[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc31b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from each book\n",
    "\n",
    "stopwords_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Stop_Words\" \n",
    "\n",
    "os.chdir(stopwords_dir)\n",
    "\n",
    "f = open(\"stop_words_english_original.txt\", \"r\", encoding=\"utf-8\")\n",
    "stopwords = []\n",
    "for text in f:\n",
    "    text = text.replace('\\n', '')\n",
    "    stopwords.append(text)\n",
    "\n",
    "\n",
    "punc = '''!()[]{};:'-\"\\,<>./?@#$%^&*_~''' # Must include \"-\" in words... or not?\n",
    "\n",
    "# Function to clean up text and remove stopwords\n",
    "def clean(book):\n",
    "    text_list = title_text[book]\n",
    "    new_text_list = []\n",
    "    \n",
    "    # Clean up text\n",
    "    for text in text_list:\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = text.split(' ')\n",
    "        new_text_list.append(text)\n",
    "\n",
    "\n",
    "    newer_text_list = []\n",
    "    \n",
    "    # Remove punctuation\n",
    "    for sentence in new_text_list:\n",
    "        for word in sentence:\n",
    "            for char in punc:\n",
    "                if char in word:\n",
    "                    word = word.replace(char, '')\n",
    "            newer_text_list.append(word)\n",
    "                \n",
    "    # Remove stop words\n",
    "    newest_text_list = []\n",
    "    for words in newer_text_list:\n",
    "        if words not in stopwords:\n",
    "            newest_text_list.append(words)\n",
    "\n",
    "    # Remove blanks\n",
    "    for w in newest_text_list:\n",
    "        if len(w) == 0:\n",
    "            newest_text_list.remove(w)\n",
    "            \n",
    "    return newest_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee1082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = clean(\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\")\n",
    "\n",
    "# print(len(set(example)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8aa26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29', 'people', 'biological', 'approach', 'traditionally', 'accepted', 'continuum', 'model', 'abnormality', 'viewing', 'psychological', 'disorders', 'schizophrenia', 'absent', 'view', 'medical', 'physical', 'disorders', 'cancer', 'biological', 'approaches', 'begun', 'view', 'disorders', 'spectrum', 'cannon', 'keller', '2006', 'disorders', 'symptoms', 'varying', 'intensity', 'autism', 'disorder', 'characterized', 'problems', 'communication', 'social', 'skills', 'activities', 'interests', 'suggests', 'spectrum', 'disorders', 'symptoms', 'vary', 'severity', 'disorder', 'severe', 'spectrum', 'aspergers', 'syndrome', 'characterized', 'problems', 'social', 'skills', 'activities', 'autism', 'severe', 'communication', 'difficulties', 'researchers', 'speak', 'autism', 'spectrum', 'disorders', 'sigman', 'spence', 'wang', '2006', 'disorders', 'spectrum', 'qualitatively', 'normal', 'functioning', '297', 'dr', 'temple', 'grandin', 'professor', 'animal', 'sciences', 'colorado', 'university', 'designed', 'onethird', 'livestockhandling', 'facilities', 'united', 'published', 'dozens', 'scientific', 'papers', 'lectures', 'lectures', 'equipment', 'procedures', 'safer', 'humane', 'animal', 'handling', 'life', 'autism', '297', 'grandin', 'classic', 'symptoms', 'autism', 'childhood', 'baby', 'desire', 'held', 'mother', 'calm', 'left', 'young', 'child', 'seldom', 'eye', 'contact', 'lack', 'people', 'frequently', 'threw', 'wild', 'tantrums', 'left', 'rocked', 'spun', 'indefinitely', 'sit', 'hours', 'beach', 'watching', 'sand', 'dribble', 'fingers', 'trancelike', '212', 'begun', 'talking', 'labeled', 'braindamaged', 'doctors', 'time', 'autism', '297', 'grandin', 'regret', 'autism', 'snap', 'fingers', 'nonautistic', 'person', 'autism', '323', 'pervasive', 'developmental', 'disorders', 'involve', 'severe', 'lasting', 'impairment', 'areas', 'development', 'including', 'social', 'interactions', 'communication', 'everyday', 'behaviors', 'interests', 'activities', 'researched', 'pervasive', 'developmental', 'disorder', 'autism', 'disorder', 'children', 'deficits', 'areas', 'dsmivtr', 'criteria', 'table', '107', 'children', 'autism', 'mild', 'level', 'mental', 'retardation', 'true', 'temple', 'grandin', 'life', 'autism', 'chapter', 'opener', 'richard', 'child', 'autism', 'range', 'deficits', 'characteristic', 'disorder', '324', 'autism', 'involves', 'types', 'deficits', 'type', 'deficits', 'social', 'interaction', 'lack', 'interaction', 'family', 'members', 'infants', 'children', 'autism', 'smile', 'coo', 'response', 'caregivers', 'initiate', 'play', 'caregivers', 'young', 'infants', 'cuddle', 'parents', 'frightened', 'infants', 'love', 'gaze', 'caregivers', 'caregivers', 'gaze', 'adoringly', 'infants', 'autism', 'eye', 'contact', 'bit', 'older', 'children', 'autism', 'interested', 'playing', 'children', 'preferring', 'remain', 'solitary', 'play', 'react', 'peoples', 'emotions', 'chapter', 'opener', 'temple', 'grandin', 'describes', 'work', 'hard', 'overcome', 'lack', 'understanding', 'social', 'interactions', '324', 'thought', 'children', 'autism', 'preoccupied', 'internal', 'thoughts', 'fantasies', 'people', 'schizophrenia', 'preoccupied', 'hallucinations', 'delusions', 'childhood', 'autism', 'considered', 'precursor', 'adult', 'schizophrenia', 'volkrnar', 'klin', '2009', 'children', 'develop', 'classic', 'symptoms', 'schizophrenia', 'hallucinations', 'delusions', 'adults', 'adults', 'schizophrenia', 'history', 'autism', 'addition', 'autism', 'schizophrenia', 'cooccur', 'high', 'rate', 'families', 'suggesting', 'disorders', 'genetic', '324', 'type', 'deficits', 'autism', 'involves', 'communication', '50', 'percent', 'children', 'autism', 'develop', 'speech', 'develop', 'language', 'children', 'previous', 'case', 'study', 'richard', 'communication', 'problems', 'children', 'autism', 'generating', 'simply', 'echoed', 'heard', 'phenomenon', 'called', 'echolalia', 'reversed', 'pronouns', 'meant', 'gene', 'ate', 'sentences', 'modulate', 'voice', 'expressiveness', 'sounding', 'voicegenerating', 'machine', '324', 'type', 'deficits', 'concerns', 'activities', 'interests', 'children', 'autism', 'engaging', 'symbolic', 'play', 'toys', 'preoccupied', 'toy', 'object', 'richard', 'preoccupied', 'miniature', 'car', 'temple', 'grandin', 'interested', 'watching', 'sand', 'dribble', 'fingers', 'engage', 'bizarre', 'repetitive', 'behaviors', 'toys', 'dolls', 'play', 'dollies', 'tea', 'child', 'autism', 'arm', 'doll', 'simply', 'pass', 'hands', 'routines', 'rituals', 'extremely', 'children', 'autism', 'aspect', 'daily', 'routine', 'changed', 'mother', 'stops', 'bank', 'school', 'fly', 'rage', 'children', 'perform', 'stereotyped', 'repetitive', 'behaviors', 'parts', 'body', 'incessantly', 'flapping', 'hands', 'banging', 'head', 'wall', 'behaviors', 'referred', 'selfstimulatory', 'behaviors', 'assumption', 'children', 'engage', 'behaviors', 'selfstimulation', 'clear', 'true', 'purpose', '325', 'table', '107', 'dsmivtr', 'criteria', 'autism', 'total', 'items', 'qualitative', 'impairment', 'social', 'interaction', 'manifested', 'marked', 'impairment', 'multiple', 'nonverbal', 'behaviors', 'eyetoeye', 'gaze', 'facial', 'expression', 'body', 'posture', 'gestures', 'regulate', 'social', 'interaction', 'failure', 'develop', 'peer', 'relationships', 'developmental', 'level', 'lack', 'spontaneous', 'seeking', 'share', 'enjoyment', 'interests', 'achievements', 'people', 'lack', 'social', 'emotional', 'reciprocity', 'qualitative', 'impairments', 'communication', 'manifested', 'delay', 'total', 'lack', 'development', 'spoken', 'language', 'accompanied', 'attempt', 'compensate', 'alternative', 'modes', 'communication', 'gesture', 'mime', 'individuals', 'adequate', 'speech', 'marked', 'inability', 'initiate', 'sustain', 'conversation', 'stereotyped', 'repetitive', 'language', 'idiosyncratic', 'language', 'lack', 'varied', 'spontaneous', 'makebelieve', 'play', 'social', 'imitative', 'play', 'developmental', 'level', 'restricted', 'repetitive', 'stereotyped', 'patterns', 'behavior', 'interests', 'activities', 'manifested', 'encompassing', 'preoccupation', 'stereotyped', 'restricted', 'patterns', 'abnormal', 'intensity', 'focus', 'inflexible', 'adherence', 'specific', 'nonfunctional', 'routines', 'rituals', 'stereotyped', 'repetitive', 'motor', 'mannerisms', 'hand', 'finger', 'flapping', 'twisting', 'complex', 'wholebody', 'movements', 'persistent', 'preoccupation', 'parts', 'objects', 'delays', 'abnormal', 'functioning', 'areas', 'onset', 'prior', 'age', '3', 'years', 'social', 'interaction', 'language', 'social', 'communication', 'symbolic', 'imaginative', 'play', '325', 'children', 'autism', 'measures', 'intellectual', 'ability', 'iq', 'tests', '50', '70', 'percent', 'children', 'showing', 'moderate', 'severe', 'intellectual', 'impairments', 'sigman', 'spence', 'wang', '2006', 'deficits', 'children', 'autism', 'confined', 'skills', 'require', 'language', 'understanding', 'points', 'view', 'score', 'average', 'range', 'subtests', 'require', 'language', 'skills', '325', 'temple', 'grandin', 'aboveaverage', 'intelligence', 'autism', 'popular', 'press', 'special', 'talents', 'children', 'autism', 'ability', 'play', 'music', 'taught', 'draw', 'extremely', 'exceptional', 'memory', 'mathematical', 'calculation', 'abilities', 'depicted', 'movie', 'rain', 'man', '325', 'definition', 'symptoms', 'autism', 'onset', 'age', '3', 'children', 'autism', 'simply', 'delayed', 'development', 'skills', 'develop', 'language', 'social', 'interaction', 'pattern', 'striking', 'deviance', 'nature', 'note', 'wide', 'variation', 'severity', 'outcome', 'disorder', 'study', '68', 'individuals', 'diagnosed', 'autism', 'children', 'performance', 'iq', '50', 'howlin', 'goode', 'hutton', 'rutter', '2004', 'adults', '13', 'sort', 'academic', 'degree', '5', 'college', '2', 'postgraduate', 'degree', '68', '23', 'employed', '18', 'close', 'friendships', 'majority', 'remained', 'dependent', 'parents', 'required', 'form', 'residential', 'care', 'fiftyeight', 'percent', '39', 'individuals', 'outcomes', 'rated', 'poor', 'poor', 'unable', 'live', 'hold', 'job', 'persistent', 'problems', 'communication', 'social', 'interactions', '326', 'rain', 'man', 'dustin', 'hoffman', 'played', 'man', 'autism', 'extraordinary', 'abilities', '326', 'predictor', 'outcome', 'autism', 'childs', 'iq', 'language', 'development', 'age', '6', 'howlin', 'al', '2004', 'nordin', 'gillberg', '1998', 'children', 'iqs', '50', 'communicative', 'speech', 'age', '6', 'prognosis', 'study', 'howlin', 'colleagues', '2004', 'people', 'iq', '70', 'achieve', 'good', 'good', 'outcome', '326', 'prevalence', 'autism', 'rising', 'years', 'increased', 'attention', 'recognition', 'disorder', 'tagerflusberg', 'joseph', 'folstein', '2001', 'review', 'epidemiological', 'studies', 'estimated', 'prevalence', 'autism', '1', '500', 'children', 'prevalence', 'forms', 'pervasive', 'developmental', 'disorder', '1', '160', 'children', 'fombonne', '2009', 'boys', 'outnumber', 'girls', 'prevalence', 'autism', 'vary', 'national', 'origin', 'race', 'ethnicity', 'socioeconomic', 'status', 'parental', 'education', '326', 'pervasive', 'developmental', 'disorders', 'recognized', 'dsmivtr', 'include', 'asperger', 'disorder', 'retts', 'disorder', 'childhood', 'disintegrative', 'disorder', 'tables', '108', '109', '326', 'aspergers', 'disorder', 'characterized', 'deficits', 'social', 'interactions', 'activities', 'interests', 'autism', 'table', '108', 'differs', 'autism', 'delays', 'deviance', 'language', '3', 'years', 'life', 'children', 'normal', 'levels', 'curiosity', 'environment', 'acquire', 'normal', 'cognitive', 'skills', 'children', 'average', 'iq', 'scores', '326', 'children', 'aspergers', 'disorder', 'tend', 'difficulty', 'social', 'relationships', 'obsessed', 'arcane', 'facts', 'issues', 'memorizing', 'zip', 'codes', 'formal', 'speechthe', 'disorder', 'referred', 'professor', 'syndrome', '326', 'prevalence', 'asperger', 'disorder', 'clear', 'individuals', 'disorder', 'function', 'life', 'undiagnosed', 'current', 'estimates', 'prevalence', '1', '36', 'people', '10000', 'volkmar', 'al', '2004', '326', 'distinctions', 'pervasive', 'developmental', 'disorders', 'autism', 'aspergers', 'disorder', 'controversial', 'volkmar', 'al', '2009', 'disorders', 'cooccur', 'families', 'clear', 'evidence', 'retts', 'childhood', 'disintegrative', 'disorder', 'disorder', 'rare', 'validity', 'diagnoses', 'questioned', 'fombonne', '2009', '327', 'table', '108', 'dsmivtr', 'criteria', 'aspergerâ€™s', 'disorder', 'qualitative', 'impairment', 'social', 'interaction', 'manifested', 'marked', 'impairment', 'multiple', 'nonverbal', 'behaviors', 'eyetoeye', 'gaze', 'facial', 'expression', 'body', 'posture', 'gestures', 'regulate', 'social', 'interaction', 'failure', 'develop', 'peer', 'relationships', 'developmental', 'level', 'lack', 'spontaneous', 'seeking', 'share', 'enjoyment', 'interests', 'achievements', 'people', 'lack', 'showing', 'bringing', 'pointing', 'objects', 'people', 'lack', 'social', 'emotional', 'reciprocity', 'restricted', 'repetitive', 'stereotyped', 'patterns', 'behavior', 'interests', 'activities', 'manifested', 'encompassing', 'preoccupation', 'stereotyped', 'restricted', 'patterns', 'abnormal', 'intensity', 'focus', 'inflexible', 'adherence', 'specific', 'nonfunctional', 'routines', 'rituals', 'stereotyped', 'repetitive', 'motor', 'mannerisms', 'hand', 'finger', 'flapping', 'twisting', 'complex', 'wholebody', 'movements', 'persistent', 'preoccupation', 'parts', 'objects', 'disturbance', 'clinically', 'impairment', 'social', 'occupational', 'areas', 'functioning', 'clinically', 'general', 'delay', 'language', 'single', 'word', 'age', '2', 'years', 'communicative', 'phrases', 'age', '3', 'years', 'clinically', 'delay', 'cognitive', 'development', 'development', 'ageappropriate', 'selfhelp', 'skills', 'adaptive', 'behavior', 'social', 'interaction', 'curiosity', 'environment', 'childhood', 'criteria', 'met', 'specific', 'pervasive', 'developmental', 'disorder', 'schizophrenia', '327', 'dsmivtr', 'recognizes', 'pervasive', 'developmental', 'disorders', 'addition', 'autism', 'aspergers', 'disorder', '328', 'dsm5', 'pervasive', 'developmental', 'disorders', 'subsuned', 'category', 'autism', 'spectrum', 'disorder', 'light', 'evidence', 'fall', 'continuum', 'severity', 'distinct', 'volkmar', 'al', '2009', 'diagnosis', 'autism', 'spectrum', 'disorder', 'require', 'children', 'evidence', 'shortfalls', 'social', 'communication', 'interactions', 'restricted', 'recurring', 'behaviors', 'dsmivtr', 'criteria', 'autism', 'aspergers', 'disorder', 'american', 'psychiatric', 'association', '2010', 'symptoms', 'early', 'childhood', 'age', 'distinctions', 'autism', 'pervasive', 'developmental', 'disorders', 'rating', 'scale', 'severity', '328', 'years', 'wide', 'variety', 'theories', 'autism', 'proposed', 'psychiatrist', 'autism', 'leo', 'kanner', '1943', 'thought', 'autism', 'caused', 'combination', 'biological', 'factors', 'poor', 'parenting', 'psychoanalytic', 'theorists', 'bettelheim', '1967', 'parents', 'children', 'autism', 'cold', 'distant', 'uncaring', 'childs', 'symptoms', 'retreat', 'secret', 'fantasies', 'response', 'unavailable', 'parents', 'decades', 'unresponsive', 'parenting', 'plays', 'role', 'development', 'autism', '328', 'biological', 'factors', 'implicated', 'development', 'autism', 'family', 'twin', 'studies', 'genetics', 'plays', 'role', 'development', 'disorder', 'siblings', 'children', 'autism', '50', 'times', 'disorder', 'siblings', 'children', 'autism', 'sigman', 'al', '2006', 'twin', 'studies', 'concordance', 'rates', 'autism', '60', 'percent', 'monozygotic', 'twins', '10', 'percent', 'dizygotic', 'twins', 'folstein', 'rosensheidley', '2001', 'addition', '90', 'percent', 'mz', 'twins', 'children', 'autism', 'cognitive', 'impairment', 'compared', '10', 'percent', 'dz', 'twins', 'children', 'autism', 'higher', 'average', 'rate', 'genetic', 'disorders', 'cognitive', 'impairment', 'including', 'fragile', 'syndrome', 'pku', 'volkmar', 'al', '2009', 'data', 'general', 'vulnerability', 'types', 'cognitive', 'impairment', 'manifested', 'autism', 'runs', 'families', 'single', 'gene', 'autism', 'abnormalities', 'genes', 'autism', 'pervasive', 'developmental', 'disorders', 'group', 'liu', 'paterson', 'szatrnari', 'autism', 'genome', 'project', 'consortium', '2008', 'sigman', 'al', '2006', '328', 'areas', 'brain', 'implicated', 'autism', '328', 'neurological', 'factors', 'play', 'role', 'autism', 'array', 'deficits', 'autism', 'suggests', 'disruption', 'normal', 'development', 'organization', 'brain', 'sigman', 'al', '2006', 'addition', '30', 'percent', 'children', 'autism', 'develop', 'seizure', 'disorders', 'adolescence', 'suggesting', 'severe', 'neurological', 'dysfunction', 'fombonne', '1999', '328', 'neuroimaging', 'studies', 'suggested', 'variety', 'structural', 'functional', 'deficits', 'brains', 'individuals', 'autism', 'including', 'cerebellum', 'cerebrum', 'amygdala', 'hippocampus', 'sigman', 'al', '2006', 'consistent', 'finding', 'greater', 'head', 'brain', 'size', 'children', 'disorder', 'children', 'lotspeich', 'al', '2004', '328', 'children', 'autism', 'pervasive', 'developmental', 'disorders', 'tasks', 'require', 'perception', 'facial', 'expressions', 'joint', 'attention', 'person', 'empathy', 'thinking', 'social', 'situations', 'abnormal', 'functioning', 'areas', 'brain', 'recruited', 'tasks', 'photos', 'faces', 'children', 'pervasive', 'developmental', 'disorder', 'activation', 'typical', 'children', 'area', 'brain', 'involved', 'facial', 'perception', 'called', 'fusiform', 'gyrus', 'figure', '10', '2', 'schultz', '2005', 'difficulty', 'perceiving', 'standing', 'facial', 'expressions', 'contribute', 'childrens', 'deficits', 'social', 'interactions', '328', 'children', 'pervasive', 'developmental', 'disorders', 'perform', 'children', 'tasks', 'requiring', 'theory', 'mindthe', 'ability', 'understand', 'people', 'including', 'oneself', 'mental', 'understanding', 'interact', 'communicate', 'baroncohen', 'swettenham', '1997', 'theory', 'mind', 'essential', 'comprehending', 'explaining', 'predicting', 'manipulating', 'behavior', 'children', 'autism', 'fail', 'tasks', 'assessing', 'theory', 'mind', 'perform', 'appropriately', 'age', 'group', 'cognitive', 'tasks', 'yirmiya', 'erel', 'shaked', 'solomonicalevi', '1998', 'deficient', 'theory', 'mind', 'difficult', 'children', 'understand', 'operate', 'social', 'communicate', 'appropriately', 'strange', 'play', 'behaviorspecifically', 'absence', 'symbolic', 'playalso', 'represent', 'inability', 'understand', 'concrete', 'realities', 'positron', 'emission', 'tomography', 'studies', 'children', 'autism', 'deficits', 'medial', 'prefrontal', 'cortex', 'amygdala', 'performing', 'theory', 'mind', 'tasks', 'castelli', 'al', '2002', '329', 'neurological', 'dysfunctions', 'result', 'genetic', 'factors', 'alternately', 'children', 'autism', 'higher', 'average', 'rate', 'prenatal', 'birth', 'complications', 'complications', 'create', 'neurological', 'damage', 'sigman', 'al', '2006', 'studies', 'differences', 'children', 'autism', 'levels', 'neurotransmitters', 'serotonin', 'dopamine', 'meaning', 'differences', 'clear', 'anderson', 'hoshino', '1997', '329', 'number', 'drugs', 'improve', 'symptoms', 'autism', 'including', 'overactivity', 'stereotyped', 'behaviors', 'headbanging', 'handflapping', 'sleep', 'disturbances', 'tension', 'kerbeshian', 'burd', 'avery', '2001', 'volkmar', '2001', 'selective', 'serotonin', 'reuptake', 'inhibitors', 'reduce', 'repetitive', 'behaviors', 'aggression', 'improve', 'social', 'interactions', 'people', 'autism', 'atypical', 'antipsychotic', 'medications', 'reduce', 'obsessive', 'repetitive', 'behaviors', 'improve', 'selfcontrol', 'naltrexone', 'drug', 'blocks', 'receptors', 'opiates', 'reducing', 'hyperactivity', 'children', 'autism', 'finally', 'stimulants', 'improve', 'attention', 'jesner', 'arefadib', 'coren', '2007', 'drugs', 'alter', 'basic', 'autistic', 'disorder', 'easier', 'people', 'autism', 'participate', 'school', 'treatment', 'interventions', '329', 'psychosocial', 'therapies', 'autism', 'combine', 'behavioral', 'techniques', 'structured', 'educational', 'services', 'koegel', 'koegel', 'brookman', '2003', 'lovaas', 'smith', '2003', 'vismara', 'rogers', '2010', 'operant', 'conditioning', 'strategies', 'reduce', 'excessive', 'behaviors', 'repetitive', 'ritualistic', 'behaviors', 'tantrums', 'aggression', 'alleviate', 'deficits', 'delays', 'deficits', 'communication', 'interactions', 'techniques', 'implemented', 'highly', 'structured', 'schools', 'designed', 'children', 'autism', 'regular', 'classrooms', 'children', 'mainstreamed', 'specific', 'deficits', 'child', 'cognitive', 'motor', 'communication', 'skills', 'targeted', 'materials', 'reduce', 'distractions', 'books', 'printed', 'bright', 'colors', 'parents', 'taught', 'implement', 'techniques', 'consistently', 'children', '329', 'pioneering', 'study', '47', 'percent', 'children', 'autism', 'intensive', 'behavioral', 'treatment', '40', 'hours', 'week', '2', 'years', 'achieved', 'normal', 'intellectual', 'educational', 'functioning', 'age', '7', 'compared', '2', 'percent', 'children', 'received', 'institutional', 'care', 'lovaas', '1987', 'studies', 'remarkable', 'improvements', 'cognitive', 'skills', 'behavioral', 'control', 'children', 'autism', 'treated', 'comprehensive', 'behavior', 'therapy', 'administered', 'parents', 'school', 'setting', 'bregman', 'gerdtz', '1997', 'koegel', 'al', '2003', 'lovaas', 'smith', '2003', 'ozonoff', 'cathcart', '1998', 'schreibman', 'charlopchristy', '1998', 'vismara', 'rogers', '2010', '329', 'figure', '10', '2', 'brain', 'activity', 'response', 'faces', 'images', 'faces', 'children', 'autism', 'image', 'activation', 'fusiform', 'gyrus', 'children', 'autism', 'image', 'left', '329', 'intensive', 'behavior', 'therapy', 'children', 'autism', 'learn', 'communication', 'social', 'skills', '330', 'test', '2', 'biological', 'factors', 'implicated', 'autism', '3', 'medications', 'treat', 'autism', '4', 'behavior', 'therapy', 'treat', 'autism', 'apply', 'harry', 'diagnosed', 'autism', 'deficits', 'deficits', 'social', 'interaction', 'deficits', 'intelligence', 'deficits', 'language', 'communication', 'deficits', 'activities', 'interests', '523', 'aspergers', 'disorder', 'pervasive', 'developmental', 'disorder', 'characterized', 'deficits', 'social', 'skills', 'activities', 'autism', 'include', 'deficits', 'language', 'cognitive', 'skills', '523', 'autism', 'childhood', 'disorder', 'marked', 'deficits', 'social', 'interaction', 'lack', 'family', 'children', 'communication', 'failing', 'modulate', 'voice', 'signify', 'emotional', 'expression', 'activities', 'interests', 'engaging', 'bizarre', 'repetitive', 'behaviors']\n"
     ]
    }
   ],
   "source": [
    "# Update all the books with their clean, stopword-less counterparts\n",
    "\n",
    "clean_texts = {}\n",
    "for book in title_text:\n",
    "    newest_text_list = clean(book)\n",
    "    clean_texts[book] = newest_text_list\n",
    "    \n",
    "print(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce1faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9962abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# total_unique_words = set()\n",
    "# for book in clean_texts:\n",
    "#     text_list = clean_texts[book]\n",
    "#     for i in text_list:\n",
    "#         if len(i) > 0 and d.check(i) and (i in web2lowerset or d.check(p.plural(i))):\n",
    "#             total_unique_words.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63584091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e9a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how_many_words_overlap = 0\n",
    "# words_that_overlap = []\n",
    "\n",
    "# for word in dictionary_clean:\n",
    "#     for word2 in total_unique_words:\n",
    "#         if word[0] == word2:\n",
    "#             how_many_words_overlap += 1\n",
    "#             words_that_overlap.append(word[0])\n",
    "            \n",
    "# percent_overlap = how_many_words_overlap/len(total_unique_words)\n",
    "            \n",
    "# print(\"How may words overlap: \", how_many_words_overlap)\n",
    "# print(\"What percentage of the textbook types overlap with the dictionary (words overlapping divided by textbook types list length): \", percent_overlap)\n",
    "# print(\"What words are these: \", words_that_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b7da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(total_unique_words)\n",
    "\n",
    "# unique_words = pd.DataFrame(total_unique_words)\n",
    "\n",
    "# unique_words.to_csv('unique_words_2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c983e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find sentiment score of the cleaned textbooks\n",
    "\n",
    "def get_sentiment(clean_textbook):\n",
    "    total_score = 0\n",
    "    for piece in clean_textbook:\n",
    "        for word in dictionary_clean:\n",
    "            if piece == word[0]: # If the word in the textbook is in the dictionary\n",
    "                total_score += word[1] # Add that word's score to the total score\n",
    "    total_score = total_score/(len(clean_textbook))\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea500af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1085142182696526\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b3fe78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Now, find sentiment score of all of the books\n",
    "\n",
    "# dmp_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\"\n",
    "\n",
    "# os.chdir(dmp_dir)\n",
    "\n",
    "# print(textbooks)\n",
    "\n",
    "# sentiments = {}\n",
    "# for textbook in textbooks:\n",
    "#     book = clean_texts[textbook]\n",
    "#     sentiment = get_sentiment(book)\n",
    "#     sentiments[textbook] = sentiment\n",
    "    \n",
    "# all_sentiments = pd.DataFrame(sentiments, index = [0])\n",
    "\n",
    "# all_sentiments.to_csv('sentiments_12_19_2023.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c34a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abn_Barlow_04_Autism_v2.docx', 'Abn_Barlow_05_Autism_v2.docx', 'Abn_Barlow_06_Autism_v2.docx', 'Abn_Barlow_07_Autism.docx', 'Abn_Barlow_08_Autism.docx', 'Abn_Brown_01_Autism_v2.docx', 'Abn_Brown_02_Autism_v2.docx', 'Abn_Brown_03_Autism_v2.docx', 'Abn_Brown_04_Autism_v2.docx', 'Abn_Brown_05_Autism.docx', 'Abn_Comer_05_Autism_v2.docx', 'Abn_Comer_06_Autism_v2.docx', 'Abn_Comer_07_Autism_v2.docx', 'Abn_Comer_08_Autism_v2.docx', 'Abn_Comer_09_Autism.docx', 'Abn_Hooley_13_Autism_v2.docx', 'Abn_Hooley_14_Autism_v2.docx', 'Abn_Hooley_15_Autism_v2.docx', 'Abn_Hooley_16_Autism_v2.docx', 'Abn_Hooley_17_Autism.docx', 'Abn_Kearney_06_Autism.docx', 'Abn_Mash_02_Autism_v2.docx', 'Abn_Mash_03_Autism_v2.docx', 'Abn_Mash_04_Autism_v2.docx', 'Abn_Mash_05_Autism_v2.docx', 'Abn_Mash_06_Autism.docx', 'Abn_Nevid_06_Autism_v2.docx', 'Abn_Nevid_07_Autism.docx', 'Abn_Nevid_08_Autism_v2.docx', 'Abn_Nevid_09_Autism_v2.docx', 'Abn_Nevid_10_Autism.docx', 'Abn_Nolen-Hoeksema_03_Autism_v2.docx', 'Abn_Nolen-Hoeksema_04_Autism.docx', 'Abn_Nolen-Hoeksema_05_Autism_v2.docx', 'Abn_Nolen-Hoeksema_06_Autism_v2.docx', 'Abn_Nolen-Hoeksema_07_Autism.docx', 'Abn_Sue_07_Autism_v2.docx', 'Abn_Sue_08_Autism_v2.docx', 'Abn_Sue_09_Autism_v2.docx', 'Abn_Sue_10_Autism_v2.docx', 'Abn_Sue_11_Autism.docx', 'Abn_Whitbourne_04_Autism_v2.docx', 'Abn_Whitbourne_05_Autism_v2.docx', 'Abn_Whitbourne_06_Autism_v2.docx', 'Abn_Whitbourne_07_Autism_v2.docx', 'Abn_Whitbourne_08_Autism.docx', 'Devo_Berger_05_Autism.docx', 'Devo_Berger_06_Autism_v2.docx', 'Devo_Berger_07_Autism.docx', 'Devo_Berger_08_Autism.docx', 'Devo_Berger_09_Autism.docx', 'Devo_Berger_10_Autism.docx', 'Devo_Berk_03_Autism.docx', 'Devo_Berk_04_Autism.docx', 'Devo_Berk_05_Autism.docx', 'Devo_Berk_06_Autism.docx', 'Devo_Berk_07_Autism.docx', 'Devo_Berk_08_Autism.docx', 'Devo_Bornstein_02_Autism.docx', 'Devo_Bornstein_03_Autism.docx', 'Devo_Bornstein_04_Autism.docx', 'Devo_Bornstein_05_Autism.docx', 'Devo_Bornstein_06_Autism.docx', 'Devo_Bornstein_07_Autism.docx', 'Devo_Feldman_04_Autism.docx', 'Devo_Feldman_05_Autism.docx', 'Devo_Feldman_06_Autism.docx', 'Devo_Feldman_07_Autism.docx', 'Devo_Feldman_08_Autism.docx', 'Devo_Kail_07_Autism.docx', 'Devo_Miller_06_Autism.docx', 'Devo_Newman_13_Autism.docx', 'Devo_Santrock_12_Autism_v2.docx', 'Devo_Santrock_13_Autism_v2.docx', 'Devo_Santrock_14_Autism_v2.docx', 'Devo_Santrock_15_Autism.docx', 'Devo_Santrock_16_Autism.docx', 'Devo_Sigelman_04_Autism_v2.docx', 'Devo_Sigelman_05_Autism.docx', 'Devo_Sigelman_06_Autism.docx', 'Devo_Sigelman_07_Autism.docx', 'Devo_Sigelman_08_Autism.docx', 'Devo_Sigelman_09_Autism.docx', 'Intro_Bernstein_06_Autism.docx', 'Intro_Bernstein_07_Autism_v2.docx', 'Intro_Bernstein_08_Autism.docx', 'Intro_Bernstein_09_Autism_v2.docx', 'Intro_Bernstein_10_Autism.docx', 'Intro_Coon_10_Autism.docx', 'Intro_Coon_11_Autism.docx', 'Intro_Coon_13_Autism_v2.docx', 'Intro_Coon_14_Autism.docx', 'Intro_Griggs_03_Autism.docx', 'Intro_Griggs_04_Autism.docx', 'Intro_Griggs_05_Autism.docx', 'Intro_Kalat_09_Autism.docx', 'Intro_Kalat_10_Autism.docx', 'Intro_Kalat_11_Autism.docx', 'Intro_Morris_07_Autism.docx', 'Intro_Morris_08_Autism.docx', 'Intro_Morris_09_Autism.docx', 'Intro_Morris_10_Autism.docx', 'Intro_Morris_11_Autism.docx', 'Intro_Myers_07_Autism_v2.docx', 'Intro_Myers_08_Autism.docx', 'Intro_Myers_09_Autism.docx', 'Intro_Myers_10_Autism.docx', 'Intro_Myers_11_Autism.docx', 'Intro_Rathus_05_Autism.docx', 'Intro_Wade_08_Autism.docx', 'Intro_Wade_12_Autism.docx', 'Intro_Weiten_06_Autism.docx', 'Intro_Weiten_08_Autism.docx', 'Intro_Weiten_10_Autism.docx', 'Neuro_Bear_03_Autism.docx', 'Neuro_Bear_04_Autism.docx', 'Neuro_Breedlove_05_Autism.docx', 'Neuro_Breedlove_06_Autism.docx', 'Neuro_Breedlove_07_Autism_v2.docx', 'Neuro_Breedlove_08_Autism.docx', 'Neuro_Carlson_09_Autism.docx', 'Neuro_Carlson_10_Autism_v2.docx', 'Neuro_Carlson_11_Autism_v2.docx', 'Neuro_Carlson_12_Autism.docx', 'Neuro_Garrett_02_Autism.docx', 'Neuro_Garrett_03_Autism.docx', 'Neuro_Garrett_04_Autism_v2.docx', 'Neuro_Garrett_05_Autism.docx', 'Neuro_Johnson_01_Autism.docx', 'Neuro_Johnson_02_Autism_v2.docx', 'Neuro_Johnson_03_Autism.docx', 'Neuro_Johnson_04_Autism.docx', 'Neuro_Kalat_10_Autism.docx', 'Neuro_Kalat_11_Autism.docx', 'Neuro_Kalat_12_Autism.docx', 'Neuro_Kolb_04_Autism.docx', 'Neuro_Kolb_05_Autism.docx', 'Neuro_Kolb_06_Autism_v2.docx', 'Neuro_Kolb_07_Autism.docx', 'Neuro_Pinel_07_Autism.docx', 'Neuro_Pinel_08_Autism.docx', 'Neuro_Pinel_09_Autism.docx', 'Neuro_Pinel_10_Autism.docx', 'Neuro_Reisberg_06_Autism.docx', 'Socl_Aronson_08_Autism.docx', 'Socl_Aronson_09_Autism.docx', 'Socl_Baumeister_04_Autism.docx', 'Socl_Branscombe_14_Autism.docx', 'Socl_Gilovich_02_Autism.docx', 'Socl_Gilovich_03_Autism.docx', 'Socl_Gilovich_04_Autism.docx', 'Socl_Gruman_03_Autism.docx', 'Socl_Myers_12_Autism.docx', 'Socl_Rogers_02_Autism.docx', 'Socl_Rogers_03_Autism.docx', 'Socl_Rogers_04_Autism.docx', 'Socl_Zastrow_08_Autism.docx', 'Socl_Zastrow_09_Autism.docx', 'Socl_Zastrow_10_Autism.docx', 'Spcl_Friend_01_Autism_v2.docx', 'Spcl_Friend_02_Autism_v2.docx', 'Spcl_Friend_03_Autism_v2.docx', 'Spcl_Friend_04_Autism_v2.docx', 'Spcl_Friend_05_Autism.docx', 'Spcl_Gargiulo_02_Autism_v2.docx', 'Spcl_Gargiulo_03_Autism_v2.docx', 'Spcl_Gargiulo_04_Autism.docx', 'Spcl_Gargiulo_05_Autism_v2.docx', 'Spcl_Gargiulo_06_Autism.docx', 'Spcl_Hardman_08_Autism_v2.docx', 'Spcl_Hardman_09_Autism_v2.docx', 'Spcl_Hardman_10_Autism_v2.docx', 'Spcl_Hardman_11_Autism.docx', 'Spcl_Hardman_12_Autism.docx', 'Spcl_Heward_07_Autism.docx', 'Spcl_Heward_08_Autism_v2.docx', 'Spcl_Heward_09_Autism_v2.docx', 'Spcl_Heward_10_Autism_v2.docx', 'Spcl_Heward_11_Autism.docx', 'Spcl_Kuder_01_Autism_v2.docx', 'Spcl_Kuder_02_Autism_v2.docx', 'Spcl_Kuder_03_Autism_v2.docx', 'Spcl_Kuder_04_Autism_v2.docx', 'Spcl_Kuder_05_Autism.docx', 'Spcl_Lewis_05_Autism.docx', 'Spcl_Lewis_06_Autism.docx', 'Spcl_Lewis_07_Autism_v2.docx', 'Spcl_Lewis_08_Autism.docx', 'Spcl_Lewis_09_Autism.docx', 'Spcl_Overton_04_Autism.docx', 'Spcl_Overton_06_Autism.docx', 'Spcl_Overton_07_Autism.docx', 'Spcl_Overton_08_Autism.docx', 'Spcl_Smith_03_Autism_v2.docx', 'Spcl_Smith_04_Autism_v2.docx', 'Spcl_Smith_05_Autism_v2.docx', 'Spcl_Smith_06_Autism_v2.docx', 'Spcl_Smith_07_Autism_v2.docx', 'Spcl_Turnbull_04_Autism_v2.docx', 'Spcl_Turnbull_05_Autism_v2.docx', 'Spcl_Turnbull_06_Autism_v2.docx', 'Spcl_Turnbull_07_Autism_v2.docx', 'Spcl_Turnbull_08_Autism.docx', 'Spcl_Vaughn_03_Autism_v2.docx', 'Spcl_Vaughn_04_Autism_v2.docx', 'Spcl_Vaughn_05_Autism_v2.docx', 'Spcl_Vaughn_06_Autism_v2.docx', 'Spcl_Vaughn_07_Autism.docx']\n"
     ]
    }
   ],
   "source": [
    "# Find raw sentiments and see if length matters\n",
    "\n",
    "def get_raw_sentiment(clean_textbook):\n",
    "    total_score = 0\n",
    "    for piece in clean_textbook:\n",
    "        for word in dictionary_clean:\n",
    "            if piece == word[0]: # If the word in the textbook is in the dictionary\n",
    "                total_score += word[1] # Add that word's score to the total score\n",
    "    return total_score\n",
    "\n",
    "# Now, find sentiment score of all of the books\n",
    "\n",
    "dmp_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\"\n",
    "\n",
    "os.chdir(dmp_dir)\n",
    "\n",
    "print(textbooks)\n",
    "\n",
    "sentiments = {}\n",
    "for textbook in textbooks:\n",
    "    book = clean_texts[textbook]\n",
    "    sentiment = get_raw_sentiment(book)\n",
    "    sentiments[textbook] = sentiment\n",
    "    \n",
    "all_sentiments = pd.DataFrame(sentiments, index = [0])\n",
    "\n",
    "all_sentiments.to_csv('raw_sentiments_01_04_2024.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494c7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
