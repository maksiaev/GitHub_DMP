{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8af1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alexander Maksiaev\n",
    "# Purpose: Sentiment analysis of textbooks using Sentiwords\n",
    "\n",
    "\n",
    "\n",
    "# SentiWords 1.1 Dictionary \n",
    "# Dictionary Source:\n",
    "# Gatti, Lorenzo, Marco Guerini, and Marco Turchi. \n",
    "# \"SentiWords: Deriving a high precision and high coverage lexicon for sentiment analysis.\" \n",
    "# IEEE Transactions on Affective Computing 7.4 (2016): 409-421."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507f9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv \n",
    "import docx\n",
    "from english_words import get_english_words_set\n",
    "import enchant\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "# nltk.download(\"sentiwordnet\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "web2lowerset = get_english_words_set(['web2'], lower=True)\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "textbook_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Textbook_Dump\"\n",
    "\n",
    "textbooks = os.listdir(textbook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01600558",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(textbook_dir)\n",
    "\n",
    "# Function to get full text\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return fullText\n",
    "\n",
    "# Dictionary filled with text for all the books, minus the titles\n",
    "title_text = {}\n",
    "for book in textbooks:\n",
    "    total_text = getText(book)\n",
    "    text_without_title = total_text[4:]\n",
    "    for piece in text_without_title:\n",
    "        if piece == '':\n",
    "            text_without_title.remove(piece) # Does not get rid of all whitespace, but ah well.\n",
    "#     text_without_title.remove('')\n",
    "    title_text[book] = text_without_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb03bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(title_text[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82391430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from each book\n",
    "\n",
    "stopwords_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\\Stop_Words\" \n",
    "\n",
    "os.chdir(stopwords_dir)\n",
    "\n",
    "f = open(\"stop_words_english_original.txt\", \"r\", encoding=\"utf-8\")\n",
    "stopwords = []\n",
    "for text in f:\n",
    "    text = text.replace('\\n', '')\n",
    "    stopwords.append(text)\n",
    "\n",
    "\n",
    "punc = '''!()[]{};:'-\"\\,<>./?@#$%^&*_~''' # Must include \"-\" in words... or not?\n",
    "\n",
    "# Function to clean up text and remove stopwords\n",
    "def clean(book):\n",
    "    text_list = title_text[book]\n",
    "    new_text_list = []\n",
    "    \n",
    "    # Clean up text\n",
    "    for text in text_list:\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = text.split(' ')\n",
    "        new_text_list.append(text)\n",
    "\n",
    "\n",
    "    newer_text_list = []\n",
    "    \n",
    "    # Remove punctuation\n",
    "    for sentence in new_text_list:\n",
    "        for word in sentence:\n",
    "            for char in punc:\n",
    "                if char in word:\n",
    "                    word = word.replace(char, '')\n",
    "            newer_text_list.append(word)\n",
    "                \n",
    "    # Remove stop words\n",
    "    newest_text_list = []\n",
    "    for words in newer_text_list:\n",
    "        if words not in stopwords:\n",
    "            newest_text_list.append(words)\n",
    "\n",
    "    # Remove blanks\n",
    "    for w in newest_text_list:\n",
    "        if len(w) == 0:\n",
    "            newest_text_list.remove(w)\n",
    "            \n",
    "    return newest_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06de2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all the books with their clean, stopword-less counterparts\n",
    "\n",
    "clean_texts = {}\n",
    "for book in title_text:\n",
    "    newest_text_list = clean(book)\n",
    "    clean_texts[book] = newest_text_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614a35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef4a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_to_sentiwordnet = {\n",
    "    \"NN\": \"n\",\n",
    "    \"VB\": \"v\",\n",
    "    \"JJ\": \"a\",\n",
    "    \"RB\": \"r\",\n",
    "}\n",
    "\n",
    "def get_sentiment(article):\n",
    "    \n",
    "#     sentences = nltk.sent_tokenize(article)\n",
    "#     sentence_words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "#     tagged_sentence_words = flatten(nltk.pos_tag_sents(sentence_words))\n",
    "    \n",
    "    word_sum = len(article)\n",
    "    \n",
    "    tagged_sentence_words = nltk.pos_tag(article)\n",
    "    \n",
    "    # Already did this\n",
    "#     # Filter stopwords\n",
    "#     tagged_sentence_words = [word for word in tagged_sentence_words if word[1] not in english_stopwords]\n",
    "    \n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    subj_scores = []\n",
    "\n",
    "    for word, pos in tagged_sentence_words:\n",
    "        \n",
    "        swn_pos = nltk_to_sentiwordnet.get(pos[:2], None)\n",
    "        \n",
    "        synsets = list(swn.senti_synsets(word.lower(), pos=swn_pos))\n",
    "\n",
    "        \n",
    "#         print(word)\n",
    "        \n",
    "#         if word == \"autism\" or word == \"autistic\" or word == \"asperger's\":\n",
    "#             word_sum -= 1\n",
    "#             swn_pos = None\n",
    "#             synsets = 0\n",
    "    \n",
    "        if swn_pos == None:\n",
    "            continue\n",
    "    \n",
    "            \n",
    "        if len(synsets) == 0:\n",
    "            continue\n",
    "    \n",
    "        #print(\"{}:\".format(word))\n",
    "        for synset in synsets[:1]:\n",
    "            pos_scores.append(synset.pos_score())\n",
    "            neg_scores.append(synset.neg_score())\n",
    "            subj_scores.append(1 - synset.obj_score())\n",
    "            \n",
    "    # We only care about positive and negative scores\n",
    "    positives = sum(pos_scores)\n",
    "    negatives = sum(neg_scores)\n",
    "            \n",
    "        \n",
    "    # Divide by length of text\n",
    "    balanced_sentiment = (positives - negatives) \n",
    "    \n",
    "    # Scale sentiment to the +4 -4 dichotomy \n",
    "    scaled_sentiment = balanced_sentiment * 4\n",
    "    \n",
    "    final_sentiment = scaled_sentiment / word_sum\n",
    "        \n",
    "#     pos_score = np.average(pos_scores, weights=subj_scores)\n",
    "#     neg_score = np.average(neg_scores, weights=subj_scores)\n",
    "#     neut_score = np.mean(subj_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930ca122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06820652173913043"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment(clean_texts[\"Abn_Nolen-Hoeksema_05_Autism_v2.docx\"])\n",
    "\n",
    "# Balance out negative and positive scores (positive, negative, objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f7b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f347e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abn_Barlow_04_Autism_v2.docx', 'Abn_Barlow_05_Autism_v2.docx', 'Abn_Barlow_06_Autism_v2.docx', 'Abn_Barlow_07_Autism.docx', 'Abn_Barlow_08_Autism.docx', 'Abn_Brown_01_Autism_v2.docx', 'Abn_Brown_02_Autism_v2.docx', 'Abn_Brown_03_Autism_v2.docx', 'Abn_Brown_04_Autism_v2.docx', 'Abn_Brown_05_Autism.docx', 'Abn_Comer_05_Autism_v2.docx', 'Abn_Comer_06_Autism_v2.docx', 'Abn_Comer_07_Autism_v2.docx', 'Abn_Comer_08_Autism_v2.docx', 'Abn_Comer_09_Autism.docx', 'Abn_Hooley_13_Autism_v2.docx', 'Abn_Hooley_14_Autism_v2.docx', 'Abn_Hooley_15_Autism_v2.docx', 'Abn_Hooley_16_Autism_v2.docx', 'Abn_Hooley_17_Autism.docx', 'Abn_Kearney_06_Autism.docx', 'Abn_Mash_02_Autism_v2.docx', 'Abn_Mash_03_Autism_v2.docx', 'Abn_Mash_04_Autism_v2.docx', 'Abn_Mash_05_Autism_v2.docx', 'Abn_Mash_06_Autism.docx', 'Abn_Nevid_06_Autism_v2.docx', 'Abn_Nevid_07_Autism.docx', 'Abn_Nevid_08_Autism_v2.docx', 'Abn_Nevid_09_Autism_v2.docx', 'Abn_Nevid_10_Autism.docx', 'Abn_Nolen-Hoeksema_03_Autism_v2.docx', 'Abn_Nolen-Hoeksema_04_Autism.docx', 'Abn_Nolen-Hoeksema_05_Autism_v2.docx', 'Abn_Nolen-Hoeksema_06_Autism_v2.docx', 'Abn_Nolen-Hoeksema_07_Autism.docx', 'Abn_Sue_07_Autism_v2.docx', 'Abn_Sue_08_Autism_v2.docx', 'Abn_Sue_09_Autism_v2.docx', 'Abn_Sue_10_Autism_v2.docx', 'Abn_Sue_11_Autism.docx', 'Abn_Whitbourne_04_Autism_v2.docx', 'Abn_Whitbourne_05_Autism_v2.docx', 'Abn_Whitbourne_06_Autism_v2.docx', 'Abn_Whitbourne_07_Autism_v2.docx', 'Abn_Whitbourne_08_Autism.docx', 'Devo_Berger_05_Autism.docx', 'Devo_Berger_06_Autism_v2.docx', 'Devo_Berger_07_Autism.docx', 'Devo_Berger_08_Autism.docx', 'Devo_Berger_09_Autism.docx', 'Devo_Berger_10_Autism.docx', 'Devo_Berk_03_Autism.docx', 'Devo_Berk_04_Autism.docx', 'Devo_Berk_05_Autism.docx', 'Devo_Berk_06_Autism.docx', 'Devo_Berk_07_Autism.docx', 'Devo_Berk_08_Autism.docx', 'Devo_Bornstein_02_Autism.docx', 'Devo_Bornstein_03_Autism.docx', 'Devo_Bornstein_04_Autism.docx', 'Devo_Bornstein_05_Autism.docx', 'Devo_Bornstein_06_Autism.docx', 'Devo_Bornstein_07_Autism.docx', 'Devo_Feldman_04_Autism.docx', 'Devo_Feldman_05_Autism.docx', 'Devo_Feldman_06_Autism.docx', 'Devo_Feldman_07_Autism.docx', 'Devo_Feldman_08_Autism.docx', 'Devo_Kail_07_Autism.docx', 'Devo_Miller_06_Autism.docx', 'Devo_Newman_13_Autism.docx', 'Devo_Santrock_12_Autism_v2.docx', 'Devo_Santrock_13_Autism_v2.docx', 'Devo_Santrock_14_Autism_v2.docx', 'Devo_Santrock_15_Autism.docx', 'Devo_Santrock_16_Autism.docx', 'Devo_Sigelman_04_Autism_v2.docx', 'Devo_Sigelman_05_Autism.docx', 'Devo_Sigelman_06_Autism.docx', 'Devo_Sigelman_07_Autism.docx', 'Devo_Sigelman_08_Autism.docx', 'Devo_Sigelman_09_Autism.docx', 'Intro_Bernstein_06_Autism.docx', 'Intro_Bernstein_07_Autism_v2.docx', 'Intro_Bernstein_08_Autism.docx', 'Intro_Bernstein_09_Autism_v2.docx', 'Intro_Bernstein_10_Autism.docx', 'Intro_Coon_10_Autism.docx', 'Intro_Coon_11_Autism.docx', 'Intro_Coon_13_Autism_v2.docx', 'Intro_Coon_14_Autism.docx', 'Intro_Griggs_03_Autism.docx', 'Intro_Griggs_04_Autism.docx', 'Intro_Griggs_05_Autism.docx', 'Intro_Kalat_09_Autism.docx', 'Intro_Kalat_10_Autism.docx', 'Intro_Kalat_11_Autism.docx', 'Intro_Morris_07_Autism.docx', 'Intro_Morris_08_Autism.docx', 'Intro_Morris_09_Autism.docx', 'Intro_Morris_10_Autism.docx', 'Intro_Morris_11_Autism.docx', 'Intro_Myers_07_Autism_v2.docx', 'Intro_Myers_08_Autism.docx', 'Intro_Myers_09_Autism.docx', 'Intro_Myers_10_Autism.docx', 'Intro_Myers_11_Autism.docx', 'Intro_Rathus_05_Autism.docx', 'Intro_Wade_08_Autism.docx', 'Intro_Wade_12_Autism.docx', 'Intro_Weiten_06_Autism.docx', 'Intro_Weiten_08_Autism.docx', 'Intro_Weiten_10_Autism.docx', 'Neuro_Bear_03_Autism.docx', 'Neuro_Bear_04_Autism.docx', 'Neuro_Breedlove_05_Autism.docx', 'Neuro_Breedlove_06_Autism.docx', 'Neuro_Breedlove_07_Autism_v2.docx', 'Neuro_Breedlove_08_Autism.docx', 'Neuro_Carlson_09_Autism.docx', 'Neuro_Carlson_10_Autism_v2.docx', 'Neuro_Carlson_11_Autism_v2.docx', 'Neuro_Carlson_12_Autism.docx', 'Neuro_Garrett_02_Autism.docx', 'Neuro_Garrett_03_Autism.docx', 'Neuro_Garrett_04_Autism_v2.docx', 'Neuro_Garrett_05_Autism.docx', 'Neuro_Johnson_01_Autism.docx', 'Neuro_Johnson_02_Autism_v2.docx', 'Neuro_Johnson_03_Autism.docx', 'Neuro_Johnson_04_Autism.docx', 'Neuro_Kalat_10_Autism.docx', 'Neuro_Kalat_11_Autism.docx', 'Neuro_Kalat_12_Autism.docx', 'Neuro_Kolb_04_Autism.docx', 'Neuro_Kolb_05_Autism.docx', 'Neuro_Kolb_06_Autism_v2.docx', 'Neuro_Kolb_07_Autism.docx', 'Neuro_Pinel_07_Autism.docx', 'Neuro_Pinel_08_Autism.docx', 'Neuro_Pinel_09_Autism.docx', 'Neuro_Pinel_10_Autism.docx', 'Neuro_Reisberg_06_Autism.docx', 'Socl_Aronson_08_Autism.docx', 'Socl_Aronson_09_Autism.docx', 'Socl_Baumeister_04_Autism.docx', 'Socl_Branscombe_14_Autism.docx', 'Socl_Gilovich_02_Autism.docx', 'Socl_Gilovich_03_Autism.docx', 'Socl_Gilovich_04_Autism.docx', 'Socl_Gruman_03_Autism.docx', 'Socl_Myers_12_Autism.docx', 'Socl_Rogers_02_Autism.docx', 'Socl_Rogers_03_Autism.docx', 'Socl_Rogers_04_Autism.docx', 'Socl_Zastrow_08_Autism.docx', 'Socl_Zastrow_09_Autism.docx', 'Socl_Zastrow_10_Autism.docx', 'Spcl_Friend_01_Autism_v2.docx', 'Spcl_Friend_02_Autism_v2.docx', 'Spcl_Friend_03_Autism_v2.docx', 'Spcl_Friend_04_Autism_v2.docx', 'Spcl_Friend_05_Autism.docx', 'Spcl_Gargiulo_02_Autism_v2.docx', 'Spcl_Gargiulo_03_Autism_v2.docx', 'Spcl_Gargiulo_04_Autism.docx', 'Spcl_Gargiulo_05_Autism_v2.docx', 'Spcl_Gargiulo_06_Autism.docx', 'Spcl_Hardman_08_Autism_v2.docx', 'Spcl_Hardman_09_Autism_v2.docx', 'Spcl_Hardman_10_Autism_v2.docx', 'Spcl_Hardman_11_Autism.docx', 'Spcl_Hardman_12_Autism.docx', 'Spcl_Heward_07_Autism.docx', 'Spcl_Heward_08_Autism_v2.docx', 'Spcl_Heward_09_Autism_v2.docx', 'Spcl_Heward_10_Autism_v2.docx', 'Spcl_Heward_11_Autism.docx', 'Spcl_Kuder_01_Autism_v2.docx', 'Spcl_Kuder_02_Autism_v2.docx', 'Spcl_Kuder_03_Autism_v2.docx', 'Spcl_Kuder_04_Autism_v2.docx', 'Spcl_Kuder_05_Autism.docx', 'Spcl_Lewis_05_Autism.docx', 'Spcl_Lewis_06_Autism.docx', 'Spcl_Lewis_07_Autism_v2.docx', 'Spcl_Lewis_08_Autism.docx', 'Spcl_Lewis_09_Autism.docx', 'Spcl_Overton_04_Autism.docx', 'Spcl_Overton_06_Autism.docx', 'Spcl_Overton_07_Autism.docx', 'Spcl_Overton_08_Autism.docx', 'Spcl_Smith_03_Autism_v2.docx', 'Spcl_Smith_04_Autism_v2.docx', 'Spcl_Smith_05_Autism_v2.docx', 'Spcl_Smith_06_Autism_v2.docx', 'Spcl_Smith_07_Autism_v2.docx', 'Spcl_Turnbull_04_Autism_v2.docx', 'Spcl_Turnbull_05_Autism_v2.docx', 'Spcl_Turnbull_06_Autism_v2.docx', 'Spcl_Turnbull_07_Autism_v2.docx', 'Spcl_Turnbull_08_Autism.docx', 'Spcl_Vaughn_03_Autism_v2.docx', 'Spcl_Vaughn_04_Autism_v2.docx', 'Spcl_Vaughn_05_Autism_v2.docx', 'Spcl_Vaughn_06_Autism_v2.docx', 'Spcl_Vaughn_07_Autism.docx']\n"
     ]
    }
   ],
   "source": [
    "# Now, find sentiment score of all of the books\n",
    "\n",
    "dmp_dir = r\"C:\\Users\\maksi\\Documents\\UVA\\Research\\DMP\"\n",
    "\n",
    "os.chdir(dmp_dir)\n",
    "\n",
    "print(textbooks)\n",
    "\n",
    "sentiments = {}\n",
    "for textbook in textbooks:\n",
    "    book = clean_texts[textbook]\n",
    "    sentiment = get_sentiment(book)\n",
    "    sentiments[textbook] = sentiment\n",
    "    \n",
    "all_sentiments = pd.DataFrame(sentiments, index = [0])\n",
    "\n",
    "all_sentiments.to_csv('sentiments_sentiwords_1_18_2023.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44255b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
